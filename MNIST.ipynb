{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1-FrF08LgzsZfTll_JxhdCyqabKu8WWb2",
      "authorship_tag": "ABX9TyOKS2axqPwKuqmiyUMLXvLu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tom271/MLforTerrainGeneration/blob/main/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P62htlvbk7sM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 100\n",
        "\n",
        "# MNIST Dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
      ],
      "metadata": {
        "id": "Sx_MQ3XGnqeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae0635b-43fb-4551-a27d-77a7ada85230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 110] Connection timed out>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 17.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 110] Connection timed out>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 484kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 110] Connection timed out>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.44MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 110] Connection timed out>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.19MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        return torch.tanh(self.fc4(x))\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        return torch.sigmoid(self.fc4(x))"
      ],
      "metadata": {
        "id": "M_iOwA9IoRDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build network\n",
        "z_dim = 100\n",
        "mnist_dim = train_dataset.train_data.size(1) * train_dataset.train_data.size(2)\n",
        "\n",
        "G = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\n",
        "D = Discriminator(mnist_dim).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_lszRkRp8i7",
        "outputId": "cd22c52d-64f1-49ef-959e-669cb078a655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/datasets/mnist.py:76: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAUxzhIqqIAX",
        "outputId": "1e899006-fa67-4ae9-a43e-168a29f0354b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Generator(\n",
              "  (fc1): Linear(in_features=100, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=512, bias=True)\n",
              "  (fc3): Linear(in_features=512, out_features=1024, bias=True)\n",
              "  (fc4): Linear(in_features=1024, out_features=784, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPkEqLavqKrD",
        "outputId": "b1ff5ab6-d63a-440d-b6c1-a72c0749c2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
              "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc4): Linear(in_features=256, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# optimizer\n",
        "lr = 0.0002\n",
        "G_optimizer = optim.Adam(G.parameters(), lr = lr)\n",
        "D_optimizer = optim.Adam(D.parameters(), lr = lr)"
      ],
      "metadata": {
        "id": "E0C376yHqOkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def D_train(x):\n",
        "    #=======================Train the discriminator=======================#\n",
        "    D.zero_grad()\n",
        "\n",
        "    # train discriminator on real\n",
        "    x_real, y_real = x.view(-1, mnist_dim), torch.ones(bs, 1)\n",
        "    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))\n",
        "\n",
        "    D_output = D(x_real)\n",
        "    D_real_loss = criterion(D_output, y_real)\n",
        "    D_real_score = D_output\n",
        "\n",
        "    # train discriminator on facke\n",
        "    z = Variable(torch.randn(bs, z_dim).to(device))\n",
        "    x_fake, y_fake = G(z), Variable(torch.zeros(bs, 1).to(device))\n",
        "\n",
        "    D_output = D(x_fake)\n",
        "    D_fake_loss = criterion(D_output, y_fake)\n",
        "    D_fake_score = D_output\n",
        "\n",
        "    # gradient backprop & optimize ONLY D's parameters\n",
        "    D_loss = D_real_loss + D_fake_loss\n",
        "    D_loss.backward()\n",
        "    D_optimizer.step()\n",
        "\n",
        "    return  D_loss.data.item()"
      ],
      "metadata": {
        "id": "yi9gwoL-qSn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def G_train(x):\n",
        "    #=======================Train the generator=======================#\n",
        "    G.zero_grad()\n",
        "\n",
        "    z = Variable(torch.randn(bs, z_dim).to(device))\n",
        "    y = Variable(torch.ones(bs, 1).to(device))\n",
        "\n",
        "    G_output = G(z)\n",
        "    D_output = D(G_output)\n",
        "    G_loss = criterion(D_output, y)\n",
        "\n",
        "    # gradient backprop & optimize ONLY G's parameters\n",
        "    G_loss.backward()\n",
        "    G_optimizer.step()\n",
        "\n",
        "    return G_loss.data.item()"
      ],
      "metadata": {
        "id": "_hdKg6XeqWTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 50\n",
        "for epoch in range(1, n_epoch+1):\n",
        "    D_losses, G_losses = [], []\n",
        "    for batch_idx, (x, _) in enumerate(train_loader):\n",
        "        D_losses.append(D_train(x))\n",
        "        G_losses.append(G_train(x))\n",
        "\n",
        "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
        "            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ke8uJlhqZWm",
        "outputId": "8d8f42e4-4e2d-419f-eafd-e289f64eec32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/50]: loss_d: 0.881, loss_g: 3.330\n",
            "[2/50]: loss_d: 1.180, loss_g: 1.399\n",
            "[3/50]: loss_d: 1.121, loss_g: 1.462\n",
            "[4/50]: loss_d: 0.962, loss_g: 2.014\n",
            "[5/50]: loss_d: 0.746, loss_g: 1.951\n",
            "[6/50]: loss_d: 0.689, loss_g: 1.927\n",
            "[7/50]: loss_d: 0.676, loss_g: 2.075\n",
            "[8/50]: loss_d: 0.617, loss_g: 2.160\n",
            "[9/50]: loss_d: 0.667, loss_g: 2.099\n",
            "[10/50]: loss_d: 0.703, loss_g: 2.064\n",
            "[11/50]: loss_d: 0.730, loss_g: 1.999\n",
            "[12/50]: loss_d: 0.758, loss_g: 1.950\n",
            "[13/50]: loss_d: 0.789, loss_g: 1.889\n",
            "[14/50]: loss_d: 0.736, loss_g: 1.996\n",
            "[15/50]: loss_d: 0.730, loss_g: 2.007\n",
            "[16/50]: loss_d: 0.762, loss_g: 1.895\n",
            "[17/50]: loss_d: 0.776, loss_g: 1.909\n",
            "[18/50]: loss_d: 0.894, loss_g: 1.641\n",
            "[19/50]: loss_d: 0.865, loss_g: 1.661\n",
            "[20/50]: loss_d: 0.852, loss_g: 1.720\n",
            "[21/50]: loss_d: 0.862, loss_g: 1.669\n",
            "[22/50]: loss_d: 0.926, loss_g: 1.549\n",
            "[23/50]: loss_d: 0.935, loss_g: 1.511\n",
            "[24/50]: loss_d: 0.931, loss_g: 1.515\n",
            "[25/50]: loss_d: 0.974, loss_g: 1.448\n",
            "[26/50]: loss_d: 0.951, loss_g: 1.465\n",
            "[27/50]: loss_d: 0.979, loss_g: 1.438\n",
            "[28/50]: loss_d: 1.005, loss_g: 1.384\n",
            "[29/50]: loss_d: 1.010, loss_g: 1.356\n",
            "[30/50]: loss_d: 1.023, loss_g: 1.308\n",
            "[31/50]: loss_d: 1.050, loss_g: 1.284\n",
            "[32/50]: loss_d: 1.045, loss_g: 1.300\n",
            "[33/50]: loss_d: 1.063, loss_g: 1.268\n",
            "[34/50]: loss_d: 1.052, loss_g: 1.292\n",
            "[35/50]: loss_d: 1.056, loss_g: 1.272\n",
            "[36/50]: loss_d: 1.064, loss_g: 1.265\n",
            "[37/50]: loss_d: 1.074, loss_g: 1.236\n",
            "[38/50]: loss_d: 1.070, loss_g: 1.253\n",
            "[39/50]: loss_d: 1.080, loss_g: 1.246\n",
            "[40/50]: loss_d: 1.096, loss_g: 1.191\n",
            "[41/50]: loss_d: 1.111, loss_g: 1.168\n",
            "[42/50]: loss_d: 1.109, loss_g: 1.161\n",
            "[43/50]: loss_d: 1.126, loss_g: 1.153\n",
            "[44/50]: loss_d: 1.109, loss_g: 1.175\n",
            "[45/50]: loss_d: 1.123, loss_g: 1.148\n",
            "[46/50]: loss_d: 1.148, loss_g: 1.114\n",
            "[47/50]: loss_d: 1.160, loss_g: 1.090\n",
            "[48/50]: loss_d: 1.149, loss_g: 1.109\n",
            "[49/50]: loss_d: 1.164, loss_g: 1.073\n",
            "[50/50]: loss_d: 1.154, loss_g: 1.106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ... (rest of your code) ...\n",
        "\n",
        "# Before calling save_image, create the 'samples' directory if it doesn't exist\n",
        "os.makedirs('./samples', exist_ok=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_z = Variable(torch.randn(bs, z_dim).to(device))\n",
        "    generated = G(test_z)\n",
        "\n",
        "    save_image(generated.view(generated.size(0), 1, 28, 28), './samples/sample_' + '.png')"
      ],
      "metadata": {
        "id": "4pZ97e5sCjSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "import helper\n",
        "\n",
        "\n",
        "torch.save(G.state_dict(), 'generator.pt')\n",
        "torch.save(D.state_dict(), 'discriminator.pt')\n",
        "\n",
        "files.download('generator.pt')\n",
        "files.download('discriminator.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "S6CzJkA8yN9E",
        "outputId": "814c0f20-7466-46d3-c6e0-2d7ec2664db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fe62ea66-bcff-40d5-ab52-f7d1c4265e04\", \"generator.pt\", 5948632)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4ec57ec3-d129-41f0-bc03-dc69ac41cb7a\", \"discriminator.pt\", 5844168)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G.state_dict = torch.load('/content/drive/MyDrive/My Folder/models/generator.pth')\n",
        "print(G.state_dict.keys())\n",
        "\n",
        "D.state_dict = torch.load('/content/drive/MyDrive/My Folder/models/discriminator.pth')\n",
        "print(D.state_dict.keys())\n",
        "\n",
        "G.load_state_dict(G.state_dict)\n",
        "D.load_state_dict(D.state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRN7x1H7zBXE",
        "outputId": "64f65d8c-baa9-4eaa-f37d-240aef9a7ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-5595e9efec18>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  G.state_dict = torch.load('/content/drive/MyDrive/My Folder/models/generator.pth')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-5595e9efec18>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  D.state_dict = torch.load('/content/drive/MyDrive/My Folder/models/discriminator.pth')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias', 'fc4.weight', 'fc4.bias'])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: continue training the model for another 50 epochs\n",
        "\n",
        "n_epoch = 50\n",
        "for epoch in range(1, n_epoch+1):\n",
        "    D_losses, G_losses = [], []\n",
        "    for batch_idx, (x, _) in enumerate(train_loader):\n",
        "        D_losses.append(D_train(x))\n",
        "        G_losses.append(G_train(x))\n",
        "\n",
        "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
        "            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6059RZ3J0K49",
        "outputId": "f8a3b619-a22e-4a72-f1e9-f480726ab0e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/50]: loss_d: 1.171, loss_g: 1.065\n",
            "[2/50]: loss_d: 1.156, loss_g: 1.087\n",
            "[3/50]: loss_d: 1.174, loss_g: 1.063\n",
            "[4/50]: loss_d: 1.184, loss_g: 1.047\n",
            "[5/50]: loss_d: 1.176, loss_g: 1.054\n",
            "[6/50]: loss_d: 1.178, loss_g: 1.046\n",
            "[7/50]: loss_d: 1.198, loss_g: 1.020\n",
            "[8/50]: loss_d: 1.195, loss_g: 1.019\n",
            "[9/50]: loss_d: 1.197, loss_g: 1.016\n",
            "[10/50]: loss_d: 1.197, loss_g: 1.021\n",
            "[11/50]: loss_d: 1.194, loss_g: 1.022\n",
            "[12/50]: loss_d: 1.211, loss_g: 0.996\n",
            "[13/50]: loss_d: 1.213, loss_g: 0.992\n",
            "[14/50]: loss_d: 1.214, loss_g: 0.994\n",
            "[15/50]: loss_d: 1.209, loss_g: 1.002\n",
            "[16/50]: loss_d: 1.217, loss_g: 0.991\n",
            "[17/50]: loss_d: 1.212, loss_g: 0.986\n",
            "[18/50]: loss_d: 1.215, loss_g: 0.997\n",
            "[19/50]: loss_d: 1.215, loss_g: 0.996\n",
            "[20/50]: loss_d: 1.212, loss_g: 0.996\n",
            "[21/50]: loss_d: 1.216, loss_g: 0.982\n",
            "[22/50]: loss_d: 1.217, loss_g: 0.981\n",
            "[23/50]: loss_d: 1.223, loss_g: 0.978\n",
            "[24/50]: loss_d: 1.224, loss_g: 0.980\n",
            "[25/50]: loss_d: 1.224, loss_g: 0.970\n",
            "[26/50]: loss_d: 1.233, loss_g: 0.968\n",
            "[27/50]: loss_d: 1.238, loss_g: 0.960\n",
            "[28/50]: loss_d: 1.237, loss_g: 0.952\n",
            "[29/50]: loss_d: 1.233, loss_g: 0.955\n",
            "[30/50]: loss_d: 1.235, loss_g: 0.964\n",
            "[31/50]: loss_d: 1.232, loss_g: 0.955\n",
            "[32/50]: loss_d: 1.238, loss_g: 0.944\n",
            "[33/50]: loss_d: 1.242, loss_g: 0.943\n",
            "[34/50]: loss_d: 1.248, loss_g: 0.936\n",
            "[35/50]: loss_d: 1.242, loss_g: 0.938\n",
            "[36/50]: loss_d: 1.249, loss_g: 0.946\n",
            "[37/50]: loss_d: 1.247, loss_g: 0.942\n",
            "[38/50]: loss_d: 1.244, loss_g: 0.945\n",
            "[39/50]: loss_d: 1.249, loss_g: 0.940\n",
            "[40/50]: loss_d: 1.255, loss_g: 0.921\n",
            "[41/50]: loss_d: 1.263, loss_g: 0.912\n",
            "[42/50]: loss_d: 1.265, loss_g: 0.907\n",
            "[43/50]: loss_d: 1.268, loss_g: 0.909\n",
            "[44/50]: loss_d: 1.259, loss_g: 0.920\n",
            "[45/50]: loss_d: 1.266, loss_g: 0.910\n",
            "[46/50]: loss_d: 1.257, loss_g: 0.923\n",
            "[47/50]: loss_d: 1.266, loss_g: 0.908\n",
            "[48/50]: loss_d: 1.262, loss_g: 0.919\n",
            "[49/50]: loss_d: 1.260, loss_g: 0.917\n",
            "[50/50]: loss_d: 1.270, loss_g: 0.906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.load('/content/drive/MyDrive/My Folder/models/generator.pth')\n",
        "\n",
        "\n",
        "torch.load('/content/drive/MyDrive/My Folder/models/discriminator.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQIUH-w_1_J7",
        "outputId": "bb190fa9-020b-4f7b-916d-1070ca46876a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-6c08b702101b>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load('/content/drive/MyDrive/My Folder/models/generator.pth')\n",
            "<ipython-input-9-6c08b702101b>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load('/content/drive/MyDrive/My Folder/models/discriminator.pth')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('fc1.weight',\n",
              "              tensor([[-0.0385,  0.0255,  0.0014,  ...,  0.0012,  0.0294, -0.0178],\n",
              "                      [-0.0200,  0.0281, -0.0315,  ...,  0.0066, -0.0217, -0.0235],\n",
              "                      [ 0.0071,  0.0094, -0.0012,  ...,  0.0033,  0.0066, -0.0258],\n",
              "                      ...,\n",
              "                      [-0.0014, -0.0232,  0.0190,  ..., -0.0072,  0.0733, -0.0065],\n",
              "                      [ 0.0049,  0.0268, -0.0631,  ...,  0.0519,  0.0049,  0.0247],\n",
              "                      [-0.0159, -0.0110, -0.0161,  ..., -0.0119,  0.0458, -0.0378]],\n",
              "                     device='cuda:0')),\n",
              "             ('fc1.bias',\n",
              "              tensor([ 0.2752,  0.4087,  0.2221,  ...,  0.3341,  0.0101, -0.2173],\n",
              "                     device='cuda:0')),\n",
              "             ('fc2.weight',\n",
              "              tensor([[ 0.0161, -0.0546,  0.0003,  ..., -0.0900, -0.0618,  0.1365],\n",
              "                      [ 0.0051,  0.0046,  0.0230,  ...,  0.0585,  0.0371,  0.0512],\n",
              "                      [ 0.0387,  0.0816, -0.0331,  ..., -0.0098, -0.0239, -0.0164],\n",
              "                      ...,\n",
              "                      [-0.0214, -0.0152,  0.0166,  ...,  0.0537,  0.0098,  0.0015],\n",
              "                      [-0.0185,  0.0354,  0.0650,  ...,  0.0564, -0.0067, -0.0718],\n",
              "                      [ 0.0941,  0.0528,  0.2135,  ...,  0.1396,  0.0196,  0.0452]],\n",
              "                     device='cuda:0')),\n",
              "             ('fc2.bias',\n",
              "              tensor([-0.1974,  0.3493,  0.0719,  0.4297, -0.1787,  0.3038,  0.2221,  0.1276,\n",
              "                       0.2313,  0.0441,  0.4266,  0.3305, -0.2583,  0.3484, -0.2450,  0.1962,\n",
              "                       0.3502,  0.3180,  0.4443, -0.4321,  0.4246,  0.3875, -0.2465,  0.3011,\n",
              "                       0.2849, -0.1393,  0.4154,  0.2935, -0.0859,  0.3979, -0.2879, -0.1520,\n",
              "                       0.3819,  0.2773,  0.3198, -0.0599,  0.3572, -0.5274, -0.1645,  0.3250,\n",
              "                      -0.0470,  0.0589,  0.2962,  0.4104, -0.3807,  0.1956, -0.3003,  0.3124,\n",
              "                      -0.1003, -0.2433,  0.2948,  0.3869,  0.2641, -0.0670,  0.2240, -0.1024,\n",
              "                       0.2976,  0.3995,  0.4240, -0.1116,  0.4315, -0.1047,  0.1832,  0.1354,\n",
              "                       0.2540, -0.0083, -0.1103,  0.0407,  0.3761, -0.0535, -0.2421, -0.2539,\n",
              "                       0.3528,  0.4281, -0.1800, -0.0439,  0.3794,  0.2376,  0.3580,  0.1362,\n",
              "                       0.4629, -0.1527,  0.1260,  0.4136,  0.1344, -0.2020, -0.1397,  0.4509,\n",
              "                       0.0487,  0.3622,  0.2540,  0.4389,  0.3584,  0.3444, -0.0607, -0.2151,\n",
              "                       0.4639,  0.4086,  0.1980,  0.0052, -0.0404, -0.2657, -0.0590,  0.4181,\n",
              "                       0.3018,  0.0676, -0.0501,  0.0364,  0.4499,  0.2185,  0.0633,  0.2164,\n",
              "                      -0.1125,  0.3985, -0.0660,  0.3758, -0.1555, -0.3818, -0.3002,  0.4065,\n",
              "                       0.0314, -0.1401,  0.0783, -0.0483,  0.4212, -0.1511,  0.4541, -0.1628,\n",
              "                       0.2901, -0.1785, -0.2796,  0.4126,  0.3161,  0.3924,  0.0135, -0.0435,\n",
              "                       0.2859, -0.2742,  0.0884,  0.1977, -0.0980,  0.0669,  0.2113, -0.0117,\n",
              "                       0.3958, -0.3907, -0.1029,  0.3089,  0.0496, -0.1604,  0.2681,  0.3273,\n",
              "                       0.1134,  0.3532, -0.1535,  0.3282, -0.1456,  0.3952,  0.3207, -0.1029,\n",
              "                       0.2145, -0.2717, -0.1950, -0.2169,  0.3088,  0.3664,  0.2966,  0.4310,\n",
              "                       0.2281, -0.0533,  0.2775,  0.4030,  0.3884,  0.1597,  0.2965, -0.2838,\n",
              "                       0.3555,  0.3618,  0.2856, -0.1078, -0.2868,  0.3522,  0.4202,  0.3955,\n",
              "                       0.3905, -0.2397,  0.2251,  0.1486,  0.4398,  0.1474,  0.4262,  0.3924,\n",
              "                       0.2514,  0.4198,  0.2860, -0.0703, -0.0165,  0.2671,  0.3794, -0.3823,\n",
              "                       0.0670,  0.3906, -0.1754, -0.2942, -0.1624,  0.1997, -0.2688, -0.0020,\n",
              "                      -0.2775,  0.3094,  0.3222, -0.1985,  0.4074,  0.3066, -0.2158,  0.2669,\n",
              "                       0.4683, -0.2966, -0.1094,  0.4290,  0.4727,  0.3172, -0.1290,  0.2868,\n",
              "                      -0.2151,  0.1789,  0.3990, -0.2913,  0.3014, -0.2992, -0.1388,  0.2827,\n",
              "                       0.4810, -0.0305,  0.3092,  0.4403,  0.2687,  0.2871,  0.3684,  0.4618,\n",
              "                       0.3521,  0.3090, -0.1033,  0.4558, -0.3239,  0.3193, -0.0920,  0.4329,\n",
              "                      -0.1293, -0.2590,  0.3486, -0.0917,  0.3026,  0.0188,  0.3229,  0.0809,\n",
              "                       0.3834, -0.1667,  0.0119,  0.2857, -0.0951, -0.2343, -0.1557,  0.2386,\n",
              "                      -0.2695,  0.4547,  0.3894, -0.1152, -0.1313, -0.4206, -0.1251,  0.2760,\n",
              "                       0.2670,  0.3497, -0.0822,  0.2428,  0.2495,  0.4895,  0.4086,  0.1942,\n",
              "                       0.2603, -0.1110, -0.4508,  0.3064,  0.0368,  0.4438, -0.2690,  0.3749,\n",
              "                       0.3385,  0.3036,  0.3152, -0.0064,  0.3878, -0.0181, -0.2755,  0.2974,\n",
              "                       0.3132, -0.0844,  0.4537,  0.5211,  0.4269, -0.2088, -0.3489, -0.1113,\n",
              "                       0.2255,  0.0707,  0.4060, -0.0543, -0.2791,  0.3498,  0.0105, -0.1232,\n",
              "                       0.4269,  0.2992,  0.3732,  0.5128, -0.4023, -0.3360,  0.4472,  0.0006,\n",
              "                       0.2221, -0.0396, -0.0096,  0.2381,  0.4205,  0.2809,  0.3421,  0.3236,\n",
              "                      -0.1792,  0.2350,  0.4165, -0.2632,  0.4280, -0.2054, -0.0619, -0.1012,\n",
              "                       0.2803,  0.3207,  0.1287,  0.2821, -0.0990, -0.1429, -0.0343, -0.1762,\n",
              "                       0.5197, -0.1454, -0.2689,  0.3083,  0.4927,  0.2468,  0.2968,  0.0611,\n",
              "                      -0.1127, -0.1244, -0.2381, -0.1882, -0.1138,  0.2834,  0.3743,  0.1489,\n",
              "                      -0.1449, -0.1337,  0.0085, -0.0585, -0.2404,  0.4497,  0.2136,  0.4463,\n",
              "                       0.0209,  0.1205, -0.0248, -0.1549,  0.4676, -0.1092,  0.3485, -0.1645,\n",
              "                       0.2476,  0.2647, -0.0457,  0.5197, -0.3091, -0.1025, -0.0293,  0.2872,\n",
              "                       0.3309, -0.1208,  0.0733,  0.1617,  0.4211, -0.0834,  0.3069,  0.2510,\n",
              "                      -0.0331, -0.0558,  0.3972, -0.1789,  0.0484,  0.0510,  0.3956, -0.2512,\n",
              "                       0.2041,  0.4004,  0.0840,  0.3886, -0.1293, -0.0766, -0.3823,  0.0529,\n",
              "                       0.0937, -0.2803, -0.0506, -0.0889,  0.3082, -0.0990,  0.3193,  0.2090,\n",
              "                       0.2233, -0.2791, -0.0903,  0.2963,  0.2975,  0.3076,  0.3069, -0.3038,\n",
              "                       0.4688,  0.0424,  0.1357, -0.4488,  0.0649, -0.0162,  0.1061, -0.2323,\n",
              "                       0.2955, -0.2003,  0.4528, -0.1459, -0.3492,  0.4487,  0.2794,  0.2383,\n",
              "                       0.1818, -0.2213,  0.3637, -0.1333, -0.1052,  0.2211,  0.2622, -0.2570,\n",
              "                       0.3657,  0.1302,  0.4274, -0.0888,  0.0236,  0.1883,  0.4995,  0.0362,\n",
              "                       0.1194, -0.2011,  0.2294,  0.4047,  0.4208, -0.1082,  0.4156,  0.2544,\n",
              "                       0.3636, -0.4426, -0.0959,  0.1883,  0.2898, -0.0463,  0.2038,  0.4378,\n",
              "                       0.3260,  0.2444, -0.1419, -0.0040,  0.4459,  0.2736, -0.2240,  0.1213,\n",
              "                       0.2884,  0.4430,  0.4040,  0.0056, -0.1976, -0.0228,  0.3229, -0.2830,\n",
              "                       0.2386,  0.1378, -0.1082, -0.0505, -0.2599,  0.1851,  0.1561,  0.2867,\n",
              "                      -0.1391, -0.0204, -0.1536,  0.1022, -0.3036,  0.2799,  0.3853,  0.5093,\n",
              "                      -0.0902,  0.3270, -0.4410,  0.0894,  0.0543,  0.0165,  0.2615,  0.3632],\n",
              "                     device='cuda:0')),\n",
              "             ('fc3.weight',\n",
              "              tensor([[ 0.0449, -0.0122, -0.0075,  ...,  0.0979, -0.0252,  0.0077],\n",
              "                      [-0.0282, -0.0061,  0.0186,  ...,  0.0135, -0.0324,  0.1074],\n",
              "                      [-0.1184,  0.0582,  0.0189,  ...,  0.0247,  0.0016,  0.0132],\n",
              "                      ...,\n",
              "                      [-0.1184, -0.0011, -0.0415,  ..., -0.0353, -0.0167, -0.0107],\n",
              "                      [-0.0329,  0.0137, -0.0799,  ..., -0.0046,  0.0447,  0.0294],\n",
              "                      [-0.0272,  0.0637, -0.0103,  ...,  0.1120, -0.0017, -0.0293]],\n",
              "                     device='cuda:0')),\n",
              "             ('fc3.bias',\n",
              "              tensor([ 0.1140, -0.3317,  0.1515, -0.0653, -0.3594, -0.3792, -0.0770, -0.1514,\n",
              "                      -0.3354,  0.3939, -0.5616, -0.5330, -0.1287,  0.1484,  0.1510,  0.0466,\n",
              "                      -0.4957, -0.3383, -0.4510, -0.3873, -0.1875, -0.5814,  0.4622, -0.0366,\n",
              "                      -0.1021, -0.3275, -0.5373,  0.3007,  0.1236, -0.4224,  0.4189, -0.5203,\n",
              "                      -0.2117, -0.0087, -0.1655, -0.0098, -0.4722, -0.0823, -0.0692, -0.4493,\n",
              "                      -0.4289, -0.1670, -0.5352, -0.3187, -0.5190, -0.0616, -0.5593, -0.4510,\n",
              "                       0.1155, -0.5736, -0.5356, -0.0032,  0.2230,  0.2611, -0.5178,  0.2066,\n",
              "                      -0.1456, -0.1093, -0.3444, -0.0882, -0.0040, -0.5570, -0.5251,  0.1397,\n",
              "                      -0.3236, -0.0135,  0.3145,  0.3927,  0.0278,  0.3506, -0.4675, -0.3365,\n",
              "                      -0.4211, -0.4585,  0.0010, -0.4356,  0.2688, -0.4230, -0.1190, -0.3628,\n",
              "                      -0.2188, -0.4623,  0.3738, -0.1218, -0.3576, -0.6052,  0.0079,  0.0246,\n",
              "                      -0.3999, -0.1840, -0.2994, -0.4927,  0.0872, -0.5346, -0.1112, -0.5146,\n",
              "                      -0.2873, -0.1233,  0.1544, -0.1047, -0.4516, -0.0721, -0.1298, -0.1152,\n",
              "                      -0.2976, -0.0883, -0.2611, -0.4695,  0.4165, -0.5563, -0.2343, -0.3911,\n",
              "                      -0.1473, -0.3094, -0.3903,  0.2790, -0.2388, -0.5182, -0.1630, -0.3998,\n",
              "                       0.0438, -0.4518,  0.3319, -0.5329, -0.4750, -0.2861, -0.3677,  0.5425,\n",
              "                      -0.4056,  0.2697, -0.0169,  0.3192, -0.4551, -0.4859, -0.3172,  0.4356,\n",
              "                      -0.4011,  0.1629, -0.0124, -0.4130, -0.4969, -0.4106, -0.4874, -0.4079,\n",
              "                       0.2510,  0.0032, -0.3517,  0.3932, -0.0699,  0.0383, -0.0739, -0.3476,\n",
              "                      -0.0750, -0.5342, -0.1067,  0.3652, -0.4492, -0.3223, -0.0451, -0.2667,\n",
              "                       0.0006,  0.1305,  0.2753, -0.0153,  0.1509,  0.0232,  0.3642,  0.3801,\n",
              "                      -0.1218,  0.1880,  0.4723,  0.3800, -0.2280, -0.0698,  0.1255,  0.0929,\n",
              "                       0.0489, -0.3169, -0.2192, -0.4340, -0.1152, -0.0916, -0.5211, -0.0143,\n",
              "                       0.3616, -0.5953, -0.5359,  0.3743, -0.5032,  0.2921, -0.5019, -0.5473,\n",
              "                      -0.1221,  0.1965,  0.2159,  0.3088, -0.5030, -0.4824,  0.1582, -0.1119,\n",
              "                      -0.0553,  0.2438, -0.4991,  0.0438,  0.2387,  0.0208,  0.3897, -0.0877,\n",
              "                      -0.2221,  0.5252, -0.0451, -0.4226, -0.4911,  0.0620,  0.0032, -0.4837,\n",
              "                      -0.5274, -0.4157, -0.0936, -0.0946,  0.0796, -0.3446,  0.0547, -0.2974,\n",
              "                       0.1945,  0.4384,  0.0596,  0.1097, -0.4163, -0.2198, -0.5166, -0.5583,\n",
              "                       0.0648, -0.1365,  0.0050, -0.2887, -0.3135, -0.2719, -0.4967, -0.4819,\n",
              "                      -0.2631, -0.0607, -0.2543, -0.1501, -0.2160,  0.0130, -0.4917, -0.4042,\n",
              "                       0.3660, -0.4129, -0.4830, -0.5186, -0.2632, -0.4685,  0.0835, -0.1188],\n",
              "                     device='cuda:0')),\n",
              "             ('fc4.weight',\n",
              "              tensor([[ 7.8624e-03,  1.2280e-02, -3.0537e-03,  1.4886e-03,  4.5999e-03,\n",
              "                        7.7003e-03, -5.8876e-03,  7.4796e-03,  1.1916e-02, -1.0676e-02,\n",
              "                        3.8714e-03,  7.0210e-03,  1.1952e-02,  2.4143e-03, -2.3223e-03,\n",
              "                        2.8903e-03,  4.7543e-03,  1.2575e-02, -8.2184e-04,  4.8027e-03,\n",
              "                        3.2685e-03,  6.2603e-03, -1.3122e-02,  3.6474e-03,  8.1827e-03,\n",
              "                        1.2071e-02,  5.3594e-03, -1.0880e-02, -1.9073e-03,  1.0507e-02,\n",
              "                       -1.1678e-02,  1.0255e-02,  9.7219e-03, -1.1502e-02,  4.9966e-03,\n",
              "                        1.1128e-02,  7.8376e-03,  7.5940e-03,  8.9142e-03,  6.1042e-03,\n",
              "                        6.5059e-03, -1.6499e-03,  6.3848e-03,  6.9560e-03,  6.7305e-03,\n",
              "                       -4.2936e-03,  4.6381e-03,  6.8469e-03, -4.4829e-03,  3.2897e-03,\n",
              "                        1.0396e-02,  9.9808e-03, -4.9718e-04, -3.5734e-03,  1.4985e-02,\n",
              "                       -6.4763e-04,  6.8011e-03,  2.2165e-03,  1.1919e-02,  4.8172e-03,\n",
              "                        5.5303e-04,  4.6677e-03,  6.3121e-04,  6.7845e-03,  1.6111e-02,\n",
              "                        3.2350e-03, -1.1489e-02, -7.2135e-03,  6.7624e-03, -5.1785e-03,\n",
              "                        4.1198e-03,  2.5367e-03,  1.3179e-02,  1.0364e-02,  8.8774e-03,\n",
              "                        5.8332e-03, -1.7892e-03,  5.9455e-03,  1.1411e-02, -2.8882e-03,\n",
              "                        1.0684e-02,  5.4410e-03, -5.6705e-03,  7.6859e-03,  7.6994e-03,\n",
              "                        4.9199e-03,  7.8403e-03,  5.7680e-03,  1.3148e-02,  2.4331e-03,\n",
              "                        3.8394e-03,  4.3734e-03,  3.5175e-04,  5.7224e-03,  4.4108e-03,\n",
              "                        2.1885e-03,  3.7797e-03,  9.7035e-03, -3.4177e-03,  8.1055e-03,\n",
              "                        8.2632e-03,  1.4921e-02, -4.4396e-03,  1.4056e-02,  3.9638e-03,\n",
              "                        8.4375e-03,  5.8264e-03,  9.8324e-03, -1.0614e-02,  6.9158e-03,\n",
              "                        2.7880e-03,  3.8990e-03,  7.5704e-03,  5.7411e-03,  5.1465e-03,\n",
              "                       -1.5713e-02,  2.5011e-03,  4.7866e-03,  2.4321e-03,  2.9989e-03,\n",
              "                       -1.9007e-03, -1.1600e-03, -1.0660e-02,  9.6045e-03,  7.0112e-03,\n",
              "                        4.8282e-03,  4.0428e-03, -1.3437e-02,  8.8671e-03, -2.1436e-03,\n",
              "                        8.1577e-03,  2.6729e-03,  4.8582e-04,  8.4337e-03,  1.5255e-03,\n",
              "                       -8.6953e-03,  9.6678e-03, -2.7690e-03,  1.2146e-02,  4.7095e-03,\n",
              "                        7.0890e-03,  7.0092e-03,  1.0930e-02,  1.7510e-02, -2.3288e-03,\n",
              "                        9.1536e-03,  7.8631e-03, -5.1411e-03,  1.3238e-02, -1.9007e-03,\n",
              "                       -4.5150e-03,  8.6052e-03,  6.1467e-03,  8.2214e-03,  8.6851e-03,\n",
              "                       -9.3918e-03,  4.2159e-03,  2.2302e-03,  8.5822e-03,  4.1075e-03,\n",
              "                        3.8225e-03,  9.2068e-04, -8.6368e-03,  9.9068e-03, -5.9694e-03,\n",
              "                        6.7968e-03, -3.4206e-03, -7.6526e-03,  3.2900e-03, -4.1486e-03,\n",
              "                       -1.1757e-02, -1.5043e-02,  1.0204e-02,  6.4980e-03, -2.1963e-06,\n",
              "                       -2.8194e-03,  5.0145e-03,  3.4431e-03,  7.8459e-03,  8.8159e-03,\n",
              "                        1.4542e-03,  6.9568e-03,  9.3539e-03,  6.4695e-04, -7.1627e-03,\n",
              "                        7.2466e-03,  4.4299e-03, -1.0496e-02,  3.4940e-03, -1.8776e-02,\n",
              "                        2.5056e-03,  8.3370e-03,  3.1115e-03, -1.2264e-02, -1.1745e-03,\n",
              "                       -4.5620e-03,  7.4497e-04,  1.1056e-02, -1.2531e-02, -1.6084e-04,\n",
              "                        5.8243e-03, -1.3511e-02,  9.7562e-03,  1.0239e-02, -4.7657e-03,\n",
              "                        1.5809e-03, -8.1323e-03,  8.1551e-03,  5.9327e-03, -9.7907e-03,\n",
              "                        6.4384e-03,  7.5422e-03,  1.2433e-02, -6.3007e-04,  9.1888e-03,\n",
              "                        7.5293e-03,  3.0421e-03, -5.5603e-04,  6.1804e-03,  8.8532e-03,\n",
              "                        1.2338e-03,  7.2447e-03, -1.6454e-03,  5.3603e-03, -8.2844e-03,\n",
              "                       -1.2373e-02,  2.6132e-03, -2.2285e-04,  1.0688e-02,  6.1194e-03,\n",
              "                        7.7453e-03, -1.9756e-03,  5.4921e-03,  8.8479e-03,  4.0264e-03,\n",
              "                        7.2089e-03,  1.2812e-02,  7.2536e-03,  1.0174e-02,  1.0128e-02,\n",
              "                        1.0758e-02,  2.8813e-03,  1.3506e-04,  1.1075e-02,  9.7225e-03,\n",
              "                        1.0989e-02,  9.4706e-03,  8.5194e-03, -7.8634e-03,  7.0376e-03,\n",
              "                        5.3685e-03,  8.0138e-03,  8.0263e-03,  7.0466e-03,  7.3357e-03,\n",
              "                        6.5319e-03]], device='cuda:0')),\n",
              "             ('fc4.bias', tensor([-0.7107], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}